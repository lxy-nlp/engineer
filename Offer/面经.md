## 整理

### 大厂

| 序号 | 题目                                                 | 答案                                                         |      |
| :--- | ---------------------------------------------------- | ------------------------------------------------------------ | ---- |
| 1    | sigmoid函数一阶二阶导数的含义                        |                                                              |      |
| 2    | 使用numpy实现下上面的两个导数                        |                                                              |      |
| 3    | 字符串中提取数字，再排下序，合并后输出字符串（快排） |                                                              |      |
| 4    | sqrt(x)两种方法                                      |                                                              |      |
| 5    | 大数加减法                                           |                                                              |      |
| 6    | 过拟合怎么处理                                       | 过拟合的原因:1）建模样本选取有误，如样本数量太少，选样方法错误，样本标签错误等，导致选取的样本数据不足以代表预定的分类规则；2）样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则；<br/>3）假设的模型无法合理存在，或者说是假设成立的条件实际并不成立；<br/>4）参数太多，模型复杂度过高；<br/>5）对于决策树模型，如果我们对于其生长没有合理的限制，其自由生长有可能使节点只包含单纯的事件数据(event)或非事件数据(no event)，使其虽然可以完美匹配（拟合）训练数据，但是无法适应其他数据集。<br/>6）对于神经网络模型：a)对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面；<br/>b)权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。<br/>**解决方法**<br>1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。<br/>2）选取合适的停止训练标准，使对机器的训练在合适的程度；<br/>3）保留验证数据集，对训练成果进行验证；<br/>4）获取额外数据进行交叉验证；<br/>5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数。L1 L2 Dropout;<br>6) 获取更多的数据 |      |
| 7    | dropout原理                                          | 我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征. dropout为0.5时出现最大正则效果 训练时相当于训练了多个模型 测试时 相当于集成了多个模型 nn.Dropout(drop_pro) drop_pro表示可能失活的概率 |      |
| 8    | adam和sgd的区别                                      |                                                              |      |
| 9    | 特征的选择方法                                       |                                                              |      |
| 10   | 分治法                                               |                                                              |      |
| 11   | 数据漂移                                             |                                                              |      |
| 12   | Bert和word2vec损失函数的区别                         |                                                              |      |
| 13   | 多线程和多进程的应用场景                             |                                                              |      |
|      | 随机森林和GBDT                                       |                                                              |      |



### 阿里健康

1. 如果数据是0-1之间均匀分布，那么期望为0.5的概率是多少？（第一题就被问蒙了，没想到要考硬核数学基础，看来要恶补基础了） 

  在提醒过程中，几次出现微积分之类的操作，最后计算期望为0.5的概率等于0 

2. 在一个圆周上任意取两个点，问这两个点之间的连线过圆心的概率是多少？ 

  可以固定一个点，只存在一种情况使得两点连线过圆心，而连线的情况有无数种，则分母是一个无穷大，分子为1，则概率为0. 

3. 在一个圆周上任意取三个点，问这三个点围成的三角形为钝角三角形的概率有多大？ 

4. 线性代数中怎么定义特征向量，特征值的？ 

5. 你在[机器学习]()哪个[算法]()中用到了特征向量或者特征值？ 

6. [二分查找]()的时间复杂度是多少？ 

7. [二分查找]()的空间复杂度是多少？ 

  为什么[二分查找]()的空间复杂度是logn，试着解释一下？（因为需要二分和递归实现，所以需要依赖[二叉树]()+栈）  

8. 快速[排序]()时间复杂度是多少？空间复杂度是多少？ 

  快排的空间复杂度试着解释一下？（快排最坏情况，[二叉树]()深度是多少？最好情况[二叉树]()深度是多少？） 

9. 数据库中索引是依赖什么数据结构？（忘完了快） 

10. 操作系统中，当进入子函数，电脑做了什么操作（保护数据并能在子函数返回后能还原，具体做了什么工作呢？） 

11. [机器学习]()中有没有遇到过过拟合？为什么会过拟合？怎么处理过拟合？ 

12. 数据分布情况会不会产生过拟合？ 

13. [机器学习]()中，你能简单区分一下，bagging 和 boosting 方法吗？ 

14. 随机森林中的如果是决策树，决策树是怎么构建的？ 

15. boosting是不是比bagging更容易过拟合？ 

16. 深度学习有没有用到归一化？用到了什么归一化？归一化的操作是什么？有什么作用？ 

17. 激活函数用到了什么激活函数？他们区别是什么？ 

18. Relu 和 sigmoid 函数相比优势在哪？ 

19. LeakyRelu 比 Relu 的优势又在哪？ 

  20. 深度学习中你用到的损失函数有什么？ 

21. log损失函数你见过除了交叉熵以外的还有其他的损失函数吗？ 

  22. [项目]()当中：简单说一下你自己的[项目]()背景，用到了什么方法，怎么想的？ 

23. 数据规模有多大？ 

  24. 怎么做嵌入的? 

25. [算法题]()： 

  给一个有序数组，可能有重复数字，给一个数字，需要实现在这个数组中找到这个数字，如果找到则返回下标，如果找不到则返回-1，如果有重复数字只需要返回任意其中一个数字的下标即可。 

  要求用[二分查找]()写，不能用递归的方式实现。



### 京东

一面 4.27 

  自我介绍 

  [项目]()介绍： 

  从[项目]()中开始询问细节： 

  为什么你的[项目]()中要用到图神经网络做嵌入？ 

  用图卷积做嵌入的好处在哪里？ 

  图卷积做嵌入和直接用普通的神经网络做嵌入的优势在哪？ 

  可以说一下图卷积主要是怎么做的吗？ 

  原先的模型为什么不好？ 

  为什么要用到负采样？ 

  你这个负采样怎么做的可以说一下嘛？ 

  你的数据有多大？ 

  你怎么处理你的数据的？ 

  你只用了历史交互数据吗？ 

  从[项目]()中发散提问： 

  假设我给你一个这样的场景，你怎么构造你的训练模型： 

  有一堆异常用户，和一堆正在做活动的商品，你有一切你想要的数据，你怎么整理数据，你怎么构造模型，你怎么构造标签，最后怎么训练，怎么预测？ 

  除了图卷积你后面还用到了什么技术？ 

  有没有和graphsage这些基本方法进行对比？ 

 

  [算法题]()目： 

  1.给一个未[排序]()的数组，输出第K大的数字。 

  首先用快排，然后输出第K大的数字。 

  询问：快排的时间复杂度是多少。 
 

  2.给一个字符数组，输出最大的不重复的连续子数组长度。 

  例如 abcdab 

  那么就输出 4 ，因为最大的连续子数组是 abcd 下一个 a就重复了 

  一个个遍历这个数组，然后我是用的一个lista存放，用另一个listb记录到目前为位置最大的长度，每遍历一个就询问lista中有没有这个字母，如果有说明重复，则不计数，listb持续记录每一位的连续最大长度，最终输出listb中最大的数字即可。 

  询问：如果要你改进你能怎么改。 

  回答：可以从时间方面，目前是n2，可以用字典去存储，这样每次查询的时间复杂度是o(1)，这样整体的时间复杂度就变成了 O(n). 

  其次可以从空间复杂度方面，不用一个list存储最大值，只需要一个变量max_value，每次连续长度和这个最大值比较，如果比这个大，那么就更新max_value即可。 
 

  二面（很快就二面了）4.28 

  从[项目]()背景到研究意义，到数据处理，再到模型选择，模型描述，最终到预测结果。 

  就[项目]()提问和发散性提问穿插在一起： 

  一开始为什么要选用NCF模型 

  在NCF模型中嵌入方式是怎么样的？ 

  为什么NCF在已经有了深层的mlp提取特征后，左边还有一个泛化的浅层的元素之间的相乘？ 

  在你的数据上，混合模型效果好还是单独模型效果好？试着分析原因。 

  为了要做元素之间相乘？ 

  最终两边提取特征后的输出为什么要做连接，而不是相乘呢？ 

  简单说一下你的图神经网络模型吧？ 

  GCN的处理过程你能简单跟我说一说吗？ 

  基于谱的和基于空间的区别是什么？他们都优缺点是什么？ 

  假设我现在是个对实时性要求很高的系统，那么我用基于谱的图卷积的缺点是什么？你能跟我分析一下吗？ 

  GCN中为什么要乘那个度矩阵的逆 

  了解Graphsage吗？能够跟我说一说它的主要过程吗？ 
 

  [算法题]()目： 

  双指针，滑动窗口题目，在小哥哥一点点提醒下做出来，写得坑坑洼洼 

  给一个数字数组，214325，给一个目标8 

  输出能够满足这个目标8 最长的子数组，使得子数组中所有数字之和等于这个目标 

  例如这里输出143 



  这里结题思路是，用左指针和右指针代表着窗口的左边界和右边界，用一个和记录窗口中的和，判断这个和如果小于目标值，则挪动右指针向右移动，直到达到数组末尾。 

  如果大于这个目标值，则左指针向右移动，并把和值减去移出去的数字。 

  这里还有一些特殊情况，需要特殊考虑。 



作者：ArthurV
链接：https://www.nowcoder.com/discuss/655843?source_id=discuss_experience_nctrack&channel=-1
来源：牛客网



## [小红书]()-推荐[算法]()

### 一面 60min

1. 编程题目： LC34.在[排序]()数组中查找元素的第一个和最后一个位置 
2. [项目]()提问：介绍基于多头注意力机制的推荐[算法]()原理，介绍ItemCF[算法]()及其具体实现 
3. 发散分析：针对ItemCF[算法]()，思考如何在实际系统中使用改进。实际业务系统中应该考察哪些测试指标来评估实验的效果(点击/曝光/点击率) 

### 二面 50min

1. 基础[算法题]()目 LeetCode120.三角形最小路径和， 分析复杂度，给出优化方向 
2. [项目]()提问：介绍参加的推荐比赛的方案设计，从召回到[排序]()，以及效果分析。 

## [腾讯]()- PCG 推荐

### 一面 60min

1.[项目]()提问
a. 实习[项目]()：FM模型：样本构建，loss weight设计，负样本的价值，与DSSM的差别；索引构建方法、模型更新周期
b. 新闻推荐相关提问：不同模型结构对比（CNN/RNN），模型评价指标、模型调优方向
c. 介绍图神经网络推荐研究
\2. 代码题目
a. 题目：[剑指offer]() 26.树的子结构 （没写出来）
b. 题目：LC113.[二叉树]()路径总和II 

### 二面 60min

1. [项目]()提问： 
2. [机器学习]()提问
   a.特征工程中如何判断特征重要度
   b.ReLU函数的特点，缺点，如何改进 
3. 编程题目： 实现string类 （构造、析构、赋值、拷贝构造函数） （写出来30%） 

### 三面 30min

1. [项目]()提问：推荐比赛[项目]()介绍， 向量召回介绍； 
2. 介绍归并[排序]()， 说明适用场景、 时间复杂度、空间复杂度 
3. 其他个人情况了解 

### hr面

聊天， 略过

## [字节跳动]() - 抖音视频业务

### 一面[挂掉] 45min

1. [项目]()提问： 介绍比赛[项目]()。 追问：xgboost与gbdt的差别 
2. [项目]()提问： 介绍推荐比赛情况， 追问：ItemCF 与 User CF的差别 [答的不够完整] 
3. 实习[项目]()提问： FM与FFM的区别 
4. 编程： 1.手撕快排 2. 实现字典序 

## [字节跳动]()- DATA- EDU

### 0412 一面 50min

- [项目]()提问： 

1. 序列推荐相关
    a. 介绍论文方法
    b. 介绍论文中的graph embedding方法
    c. graphsage和gcn的区别点/特点
    d. 模型softmax训练有无优化
    e. zhu w除了GNN以外，还有哪些方法可以做graph embedding： DeepWalk Node2vec
    f. DeepWalk node2vec 与GNN相比有什么优势
    g. 除了推荐分类任务外，还有哪些任务可以用来学习graph embedding？ 节点分类 / 图分类 / 链接预测
2. [排序]()模型提问
   a. 接触过什么样的[排序]()模型： GBD T FM deepFM
   b. FM的特征交叉怎么做的
   c. Deep&Cross模型的特点 怎么做交叉的
   d. 多任务/多目标学习了解么？ MMOE的特点 (没有答上来)
   e. 除了GNN还有什么方法做序列建模 CNN RNN Transformer
   f. 介绍Transformer结构， self-attention怎么做的， mask是怎么做的
3. 其他[机器学习]()相关提问
   a. 介绍AUC指标， ROC曲线的横纵轴是什么
   b. AUC是否对正负样本比例敏感
   c. 对于ctr/cvr中如果正负比例差异特别大，如何处理： 1. 采样 2. 调整loss weight
   d. 给一个长度为1的木棍，随机分成3段，问：构成三角形的概率是多少？1/4 
4. 编程题目：[leetcode]() 3.无重复字符的最长子串 [Medium]

### 0415 二面 [挂掉] 45min

1. [项目]()提问：
   a.简单介绍一下简历上的[项目]()
   b.介绍一下用到的推荐模型
   c.谈一下你对推荐的理解 
2. 开放问题： 教育场景下，给出海量的学生的做题情况，可以做何种任务应用设计 
3. 编程题目：
   a. 给出一个有向无环图 DAG, 出发节点和结束节点唯一，计算图中有多少个关键节点，关键节点即移除该节点，图不连通； 没有写出来
   b. 给出[二叉树]()的中序和后序遍历，求先序遍历 (自己建树做测试) 

## [华为]() - 中央软件研究院 mindspore

### 一面 60min

1. 编程题目： float转字符串， 考虑各种边界问题 
2. [项目]()提问： 介绍论文情况， 介绍新闻推荐[项目]() 

### 二面 30min

1. [项目]()介绍： 论文的创新点， 与已有工作的最大区别 
2. 对常用深度学习框架的了解情况 torch tf paddlepaddle dgl 
3. 比赛情况提问， 分工情况，模型情况 

## [美团]() - 到店 出行

### 一面 18min

1. 实习[项目]()介绍 
2. 追问：FM改进点思考 
3. 编程题目： LC560. 和为K的子数组 （前缀和） 

### 二面 20min

1. 实习[项目]()介绍， 介绍对推荐系统完整的了解： 召回 -> 初排 -> 精排 -> 后排 
2. 编程题目： 手撕快排

 



## 知识扩展

### 优化算法

[优化算法总结](https://blog.csdn.net/u010089444/article/details/76725843?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.control&dist_request_id=1619678212618_36879&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.control) 

### Coding

sqrt

```c
 float get_sqrt(float x)
 {
     float low=0, up=x, mid, now;
     mid=(low+up)/2;
     do
     {
         now=mid;        //**now保存上一次计算的值
         if(mid*mid<x)   //**mid偏小，右移
         {
             low=mid;
         }
         else       //**mid偏大，左移
         {
             up=mid;
         }
         mid=(low+up)/2;
     }while(abs(mid-now)>eps); //**两次计算的误差小于eps，mid即为所求值
     return mid;
 }


 float get_sqrt(float a)
 {
     float x, now;
     x=a;
     do
     {
         now=x;     //**now保存上一次的x值
         x=(x+a/x)/2;  //**通过迭代更新x的值使其接近解
     }while(abs(now-x)>eps); //**两次计算的误差小于eps，x即为所求值
     return x;
 }


 float InvSqrt(float x)
 {
     float xhalf = 0.5f*x;
     int i = *(int*)&x; // get bits for floating VALUE
     i = 0x5f375a86- (i>>1); // gives initial guess y0
     x = *(float*)&i; // convert bits BACK to float
     x = x*(1.5f-xhalf*x*x); // Newton step, repeating increases accuracy
     x = x*(1.5f-xhalf*x*x); // Newton step, repeating increases accuracy
 //    x = x*(1.5f-xhalf*x*x); // Newton step, repeating increases accuracy  //**可以通过减少迭代次数来用精度换取时间
     return 1/x;
 }
```





Mine面试记录



| 日期       | 公司   | 题目                                                         | 面试建议                               | 反思                                         |
| ---------- | ------ | ------------------------------------------------------------ | -------------------------------------- | -------------------------------------------- |
| 2021 03    | 腾讯   | Cart树的实现原理<br />手写LSTM                               |                                        | 机器学习不牢固                               |
| 2021 05 11 | 苏研院 | hashmap有序还是无序<br />jvm虚拟机内存<br />springboot注解<br />前后端如何交互<br />抽象类和接口的区别和定义 | java基础不牢固<br />有项目经验是加分项 | java基础知识<br />项目里用到的技术一定要熟悉 |
|            |        |                                                              |                                        |                                              |

