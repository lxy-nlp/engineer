| 序号 | 题目                                                 | 答案                                                         |      |
| :--- | ---------------------------------------------------- | ------------------------------------------------------------ | ---- |
| 1    | sigmoid函数一阶二阶导数的含义                        |                                                              |      |
| 2    | 使用numpy实现下上面的两个导数                        |                                                              |      |
| 3    | 字符串中提取数字，再排下序，合并后输出字符串（快排） |                                                              |      |
| 4    | sqrt(x)两种方法                                      |                                                              |      |
| 5    | 大数加减法                                           |                                                              |      |
| 6    | 过拟合怎么处理                                       | 过拟合的原因:1）建模样本选取有误，如样本数量太少，选样方法错误，样本标签错误等，导致选取的样本数据不足以代表预定的分类规则；2）样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则；<br/>3）假设的模型无法合理存在，或者说是假设成立的条件实际并不成立；<br/>4）参数太多，模型复杂度过高；<br/>5）对于决策树模型，如果我们对于其生长没有合理的限制，其自由生长有可能使节点只包含单纯的事件数据(event)或非事件数据(no event)，使其虽然可以完美匹配（拟合）训练数据，但是无法适应其他数据集。<br/>6）对于神经网络模型：a)对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面；<br/>b)权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。<br/>**解决方法**<br>1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。<br/>2）选取合适的停止训练标准，使对机器的训练在合适的程度；<br/>3）保留验证数据集，对训练成果进行验证；<br/>4）获取额外数据进行交叉验证；<br/>5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数。L1 L2 Dropout;<br>6) 获取更多的数据 |      |
| 7    | dropout原理                                          | 我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征. dropout为0.5时出现最大正则效果 训练时相当于训练了多个模型 测试时 相当于集成了多个模型 nn.Dropout(drop_pro) drop_pro表示可能失活的概率 |      |
| 8    | adam和sgd的区别                                      |                                                              |      |
| 9    | 特征的选择方法                                       |                                                              |      |
| 10   | 分治法                                               |                                                              |      |
| 11   | 数据漂移                                             |                                                              |      |
| 12   | Bert和word2vec损失函数的区别                         |                                                              |      |
| 13   | 多线程和多进程的应用场景                             |                                                              |      |
|      | 随机森林和GBDT                                       |                                                              |      |

## 知识扩展

### 优化算法

[优化算法总结](https://blog.csdn.net/u010089444/article/details/76725843?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.control&dist_request_id=1619678212618_36879&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.control) 

### Coding

sqrt

```c
 float get_sqrt(float x)
 {
     float low=0, up=x, mid, now;
     mid=(low+up)/2;
     do
     {
         now=mid;        //**now保存上一次计算的值
         if(mid*mid<x)   //**mid偏小，右移
         {
             low=mid;
         }
         else       //**mid偏大，左移
         {
             up=mid;
         }
         mid=(low+up)/2;
     }while(abs(mid-now)>eps); //**两次计算的误差小于eps，mid即为所求值
     return mid;
 }


 float get_sqrt(float a)
 {
     float x, now;
     x=a;
     do
     {
         now=x;     //**now保存上一次的x值
         x=(x+a/x)/2;  //**通过迭代更新x的值使其接近解
     }while(abs(now-x)>eps); //**两次计算的误差小于eps，x即为所求值
     return x;
 }


 float InvSqrt(float x)
 {
     float xhalf = 0.5f*x;
     int i = *(int*)&x; // get bits for floating VALUE
     i = 0x5f375a86- (i>>1); // gives initial guess y0
     x = *(float*)&i; // convert bits BACK to float
     x = x*(1.5f-xhalf*x*x); // Newton step, repeating increases accuracy
     x = x*(1.5f-xhalf*x*x); // Newton step, repeating increases accuracy
 //    x = x*(1.5f-xhalf*x*x); // Newton step, repeating increases accuracy  //**可以通过减少迭代次数来用精度换取时间
     return 1/x;
 }
```





面试记录



| 日期       | 公司   | 题目                                                         | 面试建议                               | 反思                                         |
| ---------- | ------ | ------------------------------------------------------------ | -------------------------------------- | -------------------------------------------- |
| 2021 03    | 腾讯   | Cart树的实现原理<br />手写LSTM                               |                                        | 机器学习不牢固                               |
| 2021 05 11 | 苏研院 | hashmap有序还是无序<br />jvm虚拟机内存<br />springboot注解<br />前后端如何交互<br />抽象类和接口的区别和定义 | java基础不牢固<br />有项目经验是加分项 | java基础知识<br />项目里用到的技术一定要熟悉 |
|            |        |                                                              |                                        |                                              |

